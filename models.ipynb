{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport shutil","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/d/harisudarsan/twitter/train_data.csv')\nval_data=pd.read_csv('/kaggle/input/d/harisudarsan/twitter/val_data.csv')\ntest_data=pd.read_csv('/kaggle/input/d/harisudarsan/twitter/test_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN=256\ntrain_batch_size=16\nval_batch_size=32\ntest_batch_size=32\nepoch=3\nlr=1e-04","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import RobertaTokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating a custom dataset ","metadata":{}},{"cell_type":"code","source":"class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.df = df\n        self.title = df['body']\n        self.targets = self.df['target'].values\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.title)\n\n    def __getitem__(self, index):\n        title = str(self.title[index])\n        title = \" \".join(title.split())\n        \n        inputs = self.tokenizer.encode_plus(\n            title,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        target = torch.tensor(self.targets[index], dtype=torch.long)\n\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n            'targets': target\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = CustomDataset(train_data, tokenizer, MAX_LEN)\nvalid_dataset = CustomDataset(val_data, tokenizer, MAX_LEN)\ntest_dataset = CustomDataset(test_data, tokenizer, MAX_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_loader = torch.utils.data.DataLoader(train_dataset, \n    batch_size=train_batch_size,\n    shuffle=True,\n    num_workers=0\n)\n\nval_data_loader = torch.utils.data.DataLoader(valid_dataset, \n    batch_size=val_batch_size,\n    shuffle=False,\n    num_workers=0\n)\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, \n    batch_size=test_batch_size,\n    shuffle=False,\n    num_workers=0\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device=torch.device('cuda')if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_ckp(checkpoint_fpath, model, optimizer):    \n    # load check point\n    checkpoint = torch.load(checkpoint_fpath)\n    # initialize state_dict from checkpoint to model\n    model.load_state_dict(checkpoint['state_dict'])\n    # initialize optimizer from checkpoint to optimizer\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    # initialize valid_loss_min from checkpoint to valid_loss_min\n    valid_loss_min = checkpoint['valid_loss_min']\n    # return model, optimizer, epoch value, min validation loss \n    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n\ndef save_ckp(state, is_best, checkpoint_path, best_model_path):\n    \"\"\"\n    state: checkpoint we want to save\n    is_best: is this the best checkpoint; min validation loss\n    checkpoint_path: path to save checkpoint\n    best_model_path: path to save best model\n    \"\"\"\n    f_path = checkpoint_path\n    # save checkpoint data to the path given, checkpoint_path\n    torch.save(state, f_path)\n    # if it is a best model, min validation loss\n    if is_best:\n        best_fpath = best_model_path\n        # copy that checkpoint file to best path given, best_model_path\n        shutil.copyfile(f_path, best_fpath)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BERT base model\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertModel\n\nclass BERTClass(torch.nn.Module):\n    def __init__(self, num_classes):\n        super(BERTClass, self).__init__()\n        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.linear = torch.nn.Linear(768, num_classes)  # Adjust 768 to match BERT hidden size\n    \n    def forward(self, input_ids, attn_mask, token_type_ids):\n        output = self.bert_model(\n            input_ids, \n            attention_mask=attn_mask, \n            token_type_ids=token_type_ids\n        )\n        last_hidden_state = output.last_hidden_state\n        pooled_output = last_hidden_state[:, 0]  # Extract the [CLS] token representation\n        pooled_output_dropout = self.dropout(pooled_output)\n        output = self.linear(pooled_output_dropout)\n        return output\n\nnum_classes = 3  \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_model = BERTClass(num_classes)\nbert_model.to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RoBERTa base model\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaModel, RobertaConfig\n\nclass RoBERTaClass(torch.nn.Module):\n    def __init__(self, num_classes):\n        super(RoBERTaClass, self).__init__()\n        self.config = RobertaConfig.from_pretrained('roberta-base')\n        self.bert_model = RobertaModel(config=self.config)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.linear = torch.nn.Linear(768, num_classes)  # Adjust 768 to match RoBERTa hidden size\n\n    def forward(self, input_ids, attn_mask, token_type_ids):\n        output = self.bert_model(\n            input_ids=input_ids,\n            attention_mask=attn_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True\n        )\n        output_dropout = self.dropout(output.pooler_output)\n        output = self.linear(output_dropout)\n        return output\n\nnum_classes = 3\nRmodel = RoBERTaClass(num_classes)\nRmodel.to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Modified RoBERTa Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaModel, RobertaConfig\n\nclass RoBERTaClass(torch.nn.Module):\n    def __init__(self, num_classes, hidden_size=768, dropout_prob=0.3):\n        super(RoBERTaClass, self).__init__()\n        self.config = RobertaConfig.from_pretrained('roberta-base')\n        self.bert_model = RobertaModel(config=self.config)\n        self.dropout = torch.nn.Dropout(dropout_prob)\n        self.fc1 = torch.nn.Linear(hidden_size, 512)  # Add first fully connected layer\n        self.fc2 = torch.nn.Linear(512, 256)  # Add second fully connected layer\n        self.linear = torch.nn.Linear(256, num_classes)  # Adjust output layer based on the number of classes\n\n    def forward(self, input_ids, attn_mask, token_type_ids):\n        output = self.bert_model(\n            input_ids=input_ids,\n            attention_mask=attn_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True\n        )\n        output_dropout = self.dropout(output.pooler_output)\n        output_fc1 = torch.relu(self.fc1(output_dropout))  # Apply ReLU activation to the first FC layer\n        output_fc2 = torch.relu(self.fc2(output_fc1))  # Apply ReLU activation to the second FC layer\n        output = self.linear(output_fc2)\n        return output\n\n# Define the number of classes for your specific task\nnum_classes = 3\n\n# Create the model with the additional layers\nRmmodel = RoBERTaClass(num_classes)\nRmmodel.to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hybrid GRU model","metadata":{}},{"cell_type":"code","source":"class HybridModel(nn.Module):\n    def __init__(self, num_classes, roberta_model, gru_hidden_size, num_gru_layers):\n        super(HybridModel, self).__init__()\n        self.roberta = roberta_model\n        self.gru = nn.GRU(roberta_model.config.hidden_size, gru_hidden_size, num_gru_layers, batch_first=True)  # Updated\n        self.fc = nn.Linear(gru_hidden_size, num_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = roberta_output.last_hidden_state\n        gru_output, _ = self.gru(sequence_output)\n        gru_output = gru_output[:, -1, :]  # Take the last hidden state\n        logits = self.fc(gru_output)\n        return logits\n\n# Set your hyperparameters\nnum_gru_layers = 2  # Number of GRU layers\nmodel = HybridModel(num_classes, roberta_model, gru_hidden_size, num_gru_layers)\nmodel.to(device)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=lr)\nval_targets=[]\nval_outputs=[]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training, validation and testing part.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom sklearn.metrics import accuracy_score, classification_report,precision_score\n\ndef train_model(n_epochs, training_loader, validation_loader, model, \n                optimizer, checkpoint_path, best_model_path):\n  \n    valid_loss_min = np.Inf\n    criterion = nn.CrossEntropyLoss()  # Updated loss function for multi-class classification\n    \n    for epoch in range(1, n_epochs + 1):\n        train_loss = 0\n        valid_loss = 0\n        model.train()\n        print('############# Epoch {}: Training Start   #############'.format(epoch))\n        \n        for batch_idx, data in enumerate(training_loader):\n            ids = data['input_ids'].to(device, dtype=torch.long)\n            mask = data['attention_mask'].to(device, dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype=torch.long)  # Use long (integer class indices) for targets\n            outputs = model(ids, mask, token_type_ids)\n            \n            optimizer.zero_grad()\n            loss = criterion(outputs, targets)  # Calculate the CrossEntropyLoss\n            loss.backward()\n            optimizer.step()\n            \n            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n    \n        print('############# Epoch {}: Training End     #############'.format(epoch))\n    \n        print('############# Epoch {}: Validation Start   #############'.format(epoch))\n        model.eval()\n        valid_loss = 0\n        correct_predictions = 0\n        total_samples = 0\n   \n        with torch.no_grad():\n            for batch_idx, data in enumerate(validation_loader, 0):    \n                ids = data['input_ids'].to(device, dtype=torch.long)\n                mask = data['attention_mask'].to(device, dtype=torch.long)\n                token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n                targets = data['targets'].to(device, dtype=torch.long)  # Use long (integer class indices) for targets\n                outputs = model(ids, mask, token_type_ids)\n                loss = criterion(outputs, targets)\n                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n\n                _, predicted_labels = torch.max(outputs, 1)\n                correct_predictions += (predicted_labels == targets).sum().item()\n                total_samples += targets.size(0)\n        validation_accuracy = correct_predictions / total_samples\n        print('Epoch: {} \\tValidation Accuracy: {:.4f} \\tAverage Validation Loss: {:.6f}'.format(\n    epoch, validation_accuracy, valid_loss))\n\n        print('############# Epoch {}: Validation End     #############'.format(epoch))\n        \n        # Calculate average losses\n        train_loss = train_loss / len(training_loader)\n        valid_loss = valid_loss / len(validation_loader)\n        \n        # Print training/validation statistics \n        print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n            epoch, train_loss, valid_loss))\n      \n        # Create checkpoint variable and add important data\n        checkpoint = {\n            'epoch': epoch + 1,\n            'valid_loss_min': valid_loss,\n            'state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict()\n        }\n        \n        # Save checkpoint\n        save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n        \n        ## TODO: save the model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                valid_loss_min, valid_loss))\n            # Save checkpoint as the best model\n            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n            valid_loss_min = valid_loss\n\n        print('############# Epoch {}  Done   #############\\n'.format(epoch))\n        \n        #Testing\n\n# Testing\n    model.eval()\n    test_preds = []\n    test_labels = []\n\n    with torch.no_grad():\n        for batch in test_data_loader:\n            input_ids = batch['input_ids'].to(device, dtype=torch.long)\n            attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n            targets = batch['targets'].to(device, dtype=torch.long)  # Use long (integer class indices) for targets\n\n            outputs = model(input_ids, attention_mask, token_type_ids)\n            predicted_probs = torch.sigmoid(outputs)\n\n            # Convert predicted probabilities to class predictions (0, 1, or 2) based on the maximum probability\n            predicted_labels = torch.argmax(predicted_probs, dim=1).cpu().numpy()\n\n            # Append predicted labels and ground truth labels to the lists\n            test_preds.extend(predicted_labels)\n            test_labels.extend(targets.cpu().numpy().tolist())\n\n# Calculate accuracy and print classification report\n    test_accuracy = accuracy_score(test_labels, test_preds)\n    test_precision = precision_score(test_labels, test_preds, average=None) \n    print(\"Test Precision:\")\n    for idx, precision in enumerate(test_precision):\n        print(f\"Class {idx}: {precision:.4f}\")\n    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n    class_names = [\"class_0\", \"class_1\", \"class_2\"]\n    print(\"Test Classification Report:\")\n    print(classification_report(test_labels, test_preds, target_names=class_names))\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ckpt_path = \"/kaggle/working//curr_ckpt\"\nbest_model_path = \"/kaggle/working/best_model.pt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BERT\ntrained_model = train_model(epoch, train_data_loader, val_data_loader, bert_model, optimizer, ckpt_path, best_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#RoBERTa\ntrained_model = train_model(epoch, train_data_loader, val_data_loader, Rmodel, optimizer, ckpt_path, best_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Modified Roberta\ntrained_model = train_model(epoch, train_data_loader, val_data_loader, Rmmodel, optimizer, ckpt_path, best_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hybrid Model\ntrained_model = train_model(epoch, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}